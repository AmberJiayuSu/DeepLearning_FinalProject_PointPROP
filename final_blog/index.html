<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #e0e0df;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: center; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 75%; /* Change this percentage as needed */
    	/*max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		/* border-left: 1px solid #DDD;
		border-right: 1px solid #DDD; */
		padding: 8px 15em 8px 15em;
		font-family: "Open Sans", sans-serif;
		font-weight: lighter;
	}
	.margin-left-block {
			font-size: 16px;
			width: 25%; /* Change this percentage as needed */
			/*max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "Open Sans", sans-serif;
			font-weight: lighter;
			padding: 5px;
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: black;
    	text-decoration: underline;
	}
	a:hover {
		color: white;
    	background-color: black;
	}

	h1 {
		font-size: 24px;
		margin-top: 4px;
		margin-bottom: 10px;
		font-family: 'Open Sans', sans-serif;
	}

	h2 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 8px;
		font-family: 'Open Sans', sans-serif;
	}

	p {
		line-height: 1.25; /* Adjust the value as needed for desired spacing */
	}

	li{
		margin-top: 4px;
		margin-bottom: 6px;
		line-height: 1.25; /* Adjust the value as needed for desired spacing */
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 80%;
    max-width: calc(100% - 200px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

	.nav-item {
		display: block;
		margin: 12px 0 4px 15px; /* top right bottom left */
	}

	.listing_items span {
		background-color: #f1f1f6;
		padding: 2px 2px;
		border-radius: 6px;
		font-weight: 400;
	}

	

</style>

	  <title>PointPROP</title>
      <meta property="og:title" content="PointPROP: Parallel Ray Occlusion Prediction for Point Clouds" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=center>
								<tr>
									<td align=center colspan=4 style="padding-bottom:1em; padding-top:3em;">
										<span style="font-size: 42px; font-family: 'Open Sans', sans-serif, monospace; font-weight: bold; /* Adds fallbacks */">PointPROP: Parallel Ray Occlusion Prediction for Point Clouds</span>
									</td>
								</tr>
								<tr>
									<td colspan=4 align=center  style="padding-bottom:3em;"><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
								<tr>
										<td align=center>
												<span style="font-size:17px"><a target="_blank" href="https://amberjiayusu.github.io">Amber Jiayu Su</a></span>
										</td>
										<td align=center>
												<span style="font-size:17px"><a target="_blank" href="https://www.linkedin.com/in/erickylecheung/">Eric Kyle Cheung</a></span>
										</td>
								</tr>
								
						</table>
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
				<b style="font-size:18px">Outline</b><br><br>
				<a href="#intro">Introduction</a><br>
				<span class="nav-item"><a href="#motivation">Motivation</a></span>
			    <span class="nav-item"><a href="#previous_work">Previous Work</a></span>
				<span class="nav-item"><a href="#task">Task</a></span><br>
				<a href="#experiments">Experiments</a><br>
				<span class="nav-item"><a href="#data">Data</a></span>
			    <span class="nav-item"><a href="#architecture">Overall Architecture</a></span>
				<span class="nav-item"><a href="#baseline_comparison">Baseline Comparison</a></span>
				<span class="nav-item"><a href="#capacity_analysis">Capacity Analysis</a></span>
				<span class="nav-item"><a href="#local_bias">Local Bias</a></span>
				<span class="nav-item"><a href="#task_specific_data_augmentation">Task-specific Data Augmentation</a></span>
				<span class="nav-item"><a href="#evaluation">Evaluation</a></span><br>
				<a href="#results">Results</a><br><br>
				<a href="#discussion">Discussion</a><br><br>
				<a href="#future_work_and_limitations">Future Work and Limitations</a><br><br>
          </div>
			</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/your_image_here.png" width=512px/>
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Introduction</h1>

				<h2 id="motivation" style="margin-top: 20px; margin-bottom: 10px;">Motivation</h2>
				<p>
					The assessment of solar potential and shading is central to climate and energy research, supporting applications such as photovoltaic yield estimation and building energy simulation. While annual metrics are common, they miss critical temporal variation: sky conditions, and solar angles, affecting peak loads and hour-level PV output. Point-in-time radiation is therefore essential for capturing these dynamics.
				<!-- </p>
				<p> -->
					Urban radiation analysis typically relies on 3D models constructed from GIS footprints and airborne LiDAR point clouds. GIS provides 2D building outlines, while LiDAR supplies dense 3D samples of surfaces. Combined, these datasets form the basis for physics-based simulations/analysis that evaluate how solar rays interact with surrounding geometry.
				</p>
				<p>
					However, relying on GIS-based surfaces and reconstructed geometries presents practical limitations: (1) Comprehensive coverage of spatial information cannot depend on GIS alone. (2) Converting raw LiDAR point clouds (rich but non-volumetric samples) into surface or volumetric geometry is computationally expensive and often fails on complex urban forms, sometimes requiring manual cleanup.  (3) Even after preprocessing, point-in-time ray-tracing remains slow and costly.
				</p>
				<p> 
					Therefore, given the increasing availability of large-scale LiDAR data that provides rich 3D context, there is a need for efficiently and directly leveraging raw point clouds to estimate solar radiation at fine temporal scales. This project works towards this by developing a deep-learning-based surrogate model that predicts point-in-time parallel (assuming solar rays are parallel) ray occlusion from unprocessed urban LiDAR point clouds.
				</p>
				<p>
					Given the specificity of this task, we are trying to explore the overarching question:
					<strong>
						"How should we design the inductive biases of a neural surrogate model so that it can accurately approximate
						a physically-defined, geometry-dependent function?"
					</strong>
					Unlike semantic tasks, directional occlusion is a physical mapping governed by local geometry, long-range interactions,
					and directional constraints. The key challenge is determining which architectural inductive biases best capture these 
					properties within a learnable surrogate model.
				</p>
				<p> 
					As deep learning becomes increasingly integrated into physics-based surrogate modeling, our investigation aims to understand how architectural choices influence performance on a highly specific geometric–physical task. We hope that these insights can contribute to a broader discussion of how to design domain-appropriate inductive biases for physics-driven applications.
				</p>

			
				<h2 id="previous_work" style="margin-top: 20px; margin-bottom: 10px;">Previous Work</h2>
				<p>
					Recent research has developed several <span style="font-weight: 500;">physical-based simulations or analytical methods</span> to estimate point-in-time parallel ray occlusion using raw point clouds. Bognár et. al. projected the points to a discretized skydome, and trained an SVM classifier to predict occlusion labels for each sky patch based on projected points <a href="#ref_1">[1]</a>. Su and Dogan developed a voxelization-based method that converts point clouds into volumetric grids, enabling efficient ray-voxel intersection tests for occlusion determination <a href="#ref_2">[2]</a>. These methods avoid explicit surface reconstruction but still require computationally intensive preprocessing steps and hand-crafted features, limiting scalability and generalization.
				</p>
				<p>
				</p>

				<p>
					TODO: WOrk in ML on point cloud in urban space understanding
				</p>
				<p>
					<span style="font-weight: 500;">Machine Learning for Point Cloud Understanding</span> has been approached through several encoder families, each offering different trade-offs in geometric fidelity, locality modeling, and scalability:<br>
					<div class="listing_items">
						<ul>
							<li>
								<span > MLP-based encoders</span> (PointNet, PointNet++) <br>
								Operate directly on unordered point sets using shared MLPs and symmetric pooling to enforce permutation invariance; PointNet++ adds hierarchical neighborhoods for local structure.
							</li>

							<li>
								<span > Transformer-based encoders</span> (Point Transformer, Point-BERT).<br>
								Use attention mechanisms to capture long-range interactions and context-aware features. These models achieve strong accuracy but may be computationally expensive for contexts containing large number of points.
							</li>

							<li>
								<span > Lightweight hierarchical encoders</span> (RandLA-Net). <br>
								Use random down-sampling and local feature aggregation to efficiently process large numbers of points while preserving local geometric detail. This class is well-suited for large urban LiDAR datasets.
							</li>

							<li>
								<span > Autoencoder-style encoders</span> (Point-VAE, Point-MAE, Point-BERT pretraining). <br>
								Learn a global latent representation by reconstructing the input point cloud. These models are effective for capturing scene-level structure and provide strong pretraining signals, but reconstruction-focused objectives focus more on semantic understanding than fine-grained geometric details needed for occlusion reasoning.
							</li>
						</ul>
					</div>

				</p>
				<p>
					Visibility prediction / ray-based occlusion learning

					(NeRV, ray classification networks, shadow detection networks) — relevant because solar radiation ≈ “sun visibility probability × intensity.”

					maybe other visibility work?
				</p>

				<h2 id="task" style="margin-top: 20px; margin-bottom: 10px;">Task</h2>
				<p>
					In short, we model this as a binary classification task and develop a surrogate model to approximate the occlusion-testing function, which determines, for each test point in \(\text{pts}_{\text{test}}\), whether a given \(\text{ray}\) is blocked by the surrounding context geometry \(\text{pts}_{\text{context}}\):
				</p>
				<p>
					\[
						f(\text{pts}_{\text{context}},\, \text{pts}_{\text{test}},\, \text{ray}) \; \in \; \{0,1\}^m
					\]
					where
					\(\text{pts}_{\text{context}} \in \mathbb{R}^{n \times 3}\),
					\(\text{pts}_{\text{test}} \in \mathbb{R}^{m \times 3}\), and
					\(\text{ray} \in \mathbb{R}^3\).
				</p>


		    </div>
		</div>

		<div class="content-margin-container" id="experiments">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Experiments</h1>
				<h2 id="data" style="margin-top: 20px; margin-bottom: 10px;">Data</h2>
				<p>
					Urban-scale LiDAR point clouds (often containing billions of points) are partitioned into fixed-size sub-patches under computational constraints. From each sub-patch, we extract (1) context points, (2) test points, (3) a solar ray direction, and (4) binary occlusion labels indicating whether each test point is blocked along that ray. These components form the input–output pairs used to train our model.
				</p>
				<p> WE need some graph here </p>
				<p>
					The ground truth is produced via a previously developed voxelization approach, applied to each point and followed by a subsequent physics based simulation <a href="#ref_1">[1]</a>. Voxelization converts points into a three dimensional grid of cells, with each volumetric cell being solid or empty. Empty cells allow solar rays to pass through. 
				</p>
				<p>
					For each context–test point pair, we run a point-in-time occlusion simulation for 512 rays uniformly sampled over the upper hemisphere, producing binary occlusion labels for every test point under each ray direction (1 for non-occluded, 0 for occluded). To reduce directional and scene-specific overfitting, we randomly sample 16 rays per block during training. The final dataset contains 1,377 training samples, 393 validation samples, and 198 test samples, generated from 123 unique context–test block pairs.
				</p>

				<h2 id="architecture" style="margin-top: 20px; margin-bottom: 10px;">Overall Architecture</h2>
				<p>
					Our model follows a modular, encoder–classifier architecture designed to jointly reason over 3D geometry and solar direction. The point cloud encoder processes both the context cloud and the test-point cloud, producing latent representations that capture local structure, occluders, and spatial relationships relevant to ray–geometry interactions. In parallel, the ray vector is mapped into a learned embedding that serves as a directional conditioning signal. These three latent features (context, test points, and solar token) are then fused and passed to a classifier that predicts per-point occlusion.
				</p>
				<div class ="listing_items"> 
					<ul>
						<li>
							<span > Point Cloud Encoder </span>
							<p>
								Given the scale and density of the urban LiDAR blocks in our dataset, we adopt a RandLA-Net–style encoder, which offers an efficient hierarchical representation that preserves local geometric structure—critical for accurate ray–occlusion prediction. Because our task is not semantic segmentation, we discard RandLA-Net’s decoder and retain only the hierarchical feature extractor. Although transformer-based or generative encoders could also be applied, many are not benchmarked on large-scale urban LiDAR and require more evaluation effort; thus, we focus on the RandLA family for this project. 
							</p>
							<p>
								The RandLA encoder operates through multiple hierarchical stages, each performing random sampling to reduce point count followed by Local Feature Aggregation (LFA), which aggregates geometric context from nearest neighbors using attentive pooling. This produces a multi-scale latent representation consisting of per-point features and their corresponding downsampled coordinates for both context and test point sets.
							</p>
							<p>
								Following the RandLA encoding, we process the downsampled coordinates using a shared positional MLP, optionally preceded by Fourier feature embedding. The resulting positional embeddings are then fused with the RandLA features (via addition), providing each point with both learned geometric features and explicit spatial encoding.
							</p>
							
						</li>

						<li>
							<span > Ray Encoder </span>
							<p>
								A simple MLP maps the 3D ray vector into the hidden-dimension embedding space, producing a solar token that captures directional information relevant to occlusion reasoning. Optional Fourier feature embedding with fewer frequencies than used for point positions can be applied to the ray vector prior to the MLP to enhance high-frequency directional encoding.
							</p>
							
						</li>

						<li>
							<span > Classification Head </span> 
							<p>
								After obtaining the context points and test points features (added positional embeddings and encoded features), and solar token, we fuse them to predict per-point occlusion. Specifically, we concatenate the solar token to the context features by broadcasting the solar token across all context points. Then we concatenate the context features with the solar token and pass through a simple fead forward MLP to produce fused context features.
							</p>
							<p>
								We then implement two prediction heads: 
							</p>
							<ol>
								<li>
									The first model is a lightweight MLP that predicts per-point occlusion using globally pooled context features. Mean and max pooling are applied to the context features to form a compact scene-level embedding, which is broadcast to all test points and concatenated with their features before passing through a small MLP for prediction.
									<!-- The first is a lightweight MLP classifier that directly predicts per-point occlusion from the fused test-point features. We first extract the context features by applying mean pooling and max pooling over context-point features. The pooled features are concatenated and projected into a compact scene-level embedding, which is then broadcast to every test point. Each test-point feature is concatenated with this global context descriptor and passed through a small MLP to produce the final occlusion prediction. -->
								</li>
								<li>
									The third prediction head incorporates cross-attention between context and test points to enable more explicit geometric reasoning. Here, we treat the test-point features as queries and the fused context features as keys and values in a multi-head attention mechanism. This allows each test point to attend to relevant context points based on learned geometric relationships and the solar direction. The attention output is then concatenated with the original test-point features and passed through an MLP to produce the final occlusion prediction.
								</li>
							</ol>
						</li>

					</ul>
					<p>We use binary cross-entropy loss to supervise the occlusion predictions against ground-truth labels.</p>
						\[
						\mathcal{L}_{\text{BCEWithLogits}}(x, y)
						=
						-\Big[
						y \,\log \sigma(x)
						+
						(1 - y)\,\log\!\left(1 - \sigma(x)\right)
						\Big]
						\]
				</div>		
				
				<h2 id="baseline_comparison" style="margin-top:20px; margin-bottom:2px;">
					Baseline Comparison
				</h2>
				<p style="font-size:16px; color:#666; margin-top:0; margin-bottom:2px;">
					Baseline Evaluation of Architectural Inductive Biases (Transformer vs. MLP, ± Fourier)
				</p>
				<p><span style="font-weight: 500;">Set Up</span></p>
				<p>
				First, we establish a baseline comparison to examine the inductive biases of different architectural families. We evaluate transformer-based and MLP-based encoders, each tested with and without Fourier positional embeddings. This setup allows us to assess how these modeling choices affect the model’s ability to learn a geometry-dependent occlusion function. Transformers are expected to capture longer-range interactions and contextual relationships, whereas MLPs with global pooling primarily learn coarse global structure and are less effective at modeling fine geometric detail. Adding Fourier features introduces high-frequency positional encoding that can improve sensitivity to subtle geometric variations.
				</p>
				<p>
				For the transformer-based encoder, we implement a 4-head attention module with a single cross-attention block. For the MLP-based encoder, we use a 5-layer MLP with hidden dimension 128. For comparison with spatially aware point-based architectures, we also include a RandLA-Net encoder with three layers and a downsampling ratio of 2 at each layer (8× total reduction), using a final feature dimension of 256.
				</p>
				<p>
				All models are trained for 20 epochs with batch size 8 using the Adam optimizer (learning rate \(1\times10^{-4}\), weight decay \(1\times10^{-5}\)). A <code>ReduceLROnPlateau</code> scheduler (mode = "min") halves the learning rate when the validation loss does not improve for 5 epochs. We evaluate performance using accuracy, precision, recall, and F1-score on the validation and test sets.
				</p>
				<p><span style="font-weight: 500;">Results</span></p>
				<figure style="text-align: center;">
				<img src="./images/training_validation_loss_comparison.png" width="80%" />
				<figcaption><span style="font-weight: 500;">Figure X.</span> Training and validation loss curves for the four model architectures.</figcaption>
				</figure>
				<p>
				



				<h2 id="capacity_analysis" style="margin-top: 20px; margin-bottom: 2px;">Capacity Analysis</h2>
				<p style="font-size:16px; color:#666; margin-top:0; margin-bottom:10px;">
					Capacity and Resolution Analysis via Downsampling and Feature Reduction
				</p>
				<p>
				For the transformer-based encoder, we implement a 4-head attention module with a single cross-attention block. For the MLP-based encoder, we use a 5-layer MLP with hidden dimension 128. For comparison with spatially aware point-based architectures, we also include a RandLA-Net encoder with three layers and a downsampling ratio of 2 at each layer (8× total reduction), using a final feature dimension of 256.
				</p>

				<h2 id="local_bias" style="margin-top: 20px; margin-bottom: 2px;">Local Bias</h2>
				<p style="font-size:16px; color:#666; margin-top:0; margin-bottom:10px;">
					Incorporating Local Geometric Bias for Fine-grained Accuracy
				</p>

				  	
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Another section</h1>
            In this section we embed a video:
						<video class='my-video' loop autoplay muted style="width: 725px">
								<source src="./images/mtsh.mp4" type="video/mp4">
						</video>
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and limitations</h1>
						Let's end with some discussion of the implications and limitations.
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a target="_blank" href="https://doi.org/10.1080/19401493.2021.1971765">Calculating Solar Irradiance without Shading Geometry: a Point Cloud-based Method</a>, Bognár, Loonen, and Hensen, 2021<br><br>
							<a id="ref_2"></a>[2] <a target="_blank" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5611022">Voxel Shader – An Efficient Shadow Calculation Approach For Urban Building Energy Modeling</a>, Su, and Dogan, 2025<br><br>
							<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
						</div>
		    </div>
		</div>

	</body>

</html>



