<html>
<head>

<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.1/jquery.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>


<link rel="shortcut icon" href="images/icon.ico">
<style type="text/css">
	body {
		background-color: #e0e0df;
	}

	/* Hide both math displays initially, will display based on JS detection */
  .mathjax-mobile, .mathml-non-mobile { display: none; }

  /* Show the MathML content by default on non-mobile devices */
  .show-mathml .mathml-non-mobile { display: block; }
  .show-mathjax .mathjax-mobile { display: block; }

	.content-margin-container {
		display: flex;
		width: 100%; /* Ensure the container is full width */
		justify-content: center; /* Horizontally centers the children in the container */
		align-items: center;  /* Vertically centers the children in the container */
	}
	.main-content-block {
		width: 75%; /* Change this percentage as needed */
    	/*max-width: 1100px; /* Optional: Maximum width */
		background-color: #fff;
		/* border-left: 1px solid #DDD;
		border-right: 1px solid #DDD; */
		padding: 8px 15em 8px 15em;
		font-family: "Open Sans", sans-serif;
		font-weight: lighter;
	}
	.margin-left-block {
			font-size: 16px;
			width: 25%; /* Change this percentage as needed */
			/*max-width: 130px; /* Optional: Maximum width */
			position: relative;
			margin-left: 10px;
			text-align: left;
			font-family: "Open Sans", sans-serif;
			font-weight: lighter;
			padding: 5px;
	}

	img {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	.my-video {
			max-width: 100%; /* Make sure it fits inside the container */
			height: auto;
			display: block;
			margin: auto;
	}
	/* Hide both video displays initially, will display based on JS detection */
  .vid-mobile, .vid-non-mobile { display: none; }

  /* Show the video content by default on non-mobile devices */
  .show-vid-mobile .vid-mobile { display: block; }
  .show-vid-non-mobile .vid-non-mobile { display: block; }

	a:link,a:visited
	{
		color: black;
    	text-decoration: underline;
	}
	a:hover {
		color: white;
    	background-color: black;
	}

	h1 {
		font-size: 24px;
		margin-top: 4px;
		margin-bottom: 10px;
		font-family: 'Open Sans', sans-serif;
	}

	h2 {
		font-size: 18px;
		margin-top: 4px;
		margin-bottom: 8px;
		font-family: 'Open Sans', sans-serif;
	}

	p {
		line-height: 1.25; /* Adjust the value as needed for desired spacing */
		margin-top: 0.8em;
		margin-bottom: 0.8em;
		font-size: 16px;
	}

	li{
		margin-top: 4px;
		margin-bottom: 6px;
		line-height: 1.25; /* Adjust the value as needed for desired spacing */
	}

	th{
		font-weight: 500;
	}

	table.header {
    font-weight: 300;
    font-size: 17px;
    flex-grow: 1;
		width: 80%;
    max-width: calc(100% - 200px); /* Adjust according to the width of .paper-code-tab */
	}
	table td, table td * {
	    vertical-align: middle;
	    position: relative;
	}
	table.paper-code-tab {
	    flex-shrink: 0;
	    margin-left: 8px;
	    margin-top: 8px;
	    padding: 0px 0px 0px 8px;
	    width: 290px;
	    height: 150px;
	}

	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		        0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		        5px 5px 0 0px #fff, /* The second layer */
		        5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		        10px 10px 0 0px #fff, /* The third layer */
		        10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}

	hr {
    height: 1px; /* Sets the height of the line to 1 pixel */
    border: none; /* Removes the default border */
    background-color: #DDD; /* Sets the line color to black */
  }

	div.hypothesis {
		width: 80%;
		background-color: #EEE;
		border: 1px solid black;
		border-radius: 10px;
		-moz-border-radius: 10px;
		-webkit-border-radius: 10px;
		font-family: Courier;
		font-size: 18px;
		text-align: center;
		margin: auto;
		padding: 16px 16px 16px 16px;
	}

	div.citation {
    font-size: 0.8em;
    background-color:#fff;
    padding: 10px;
		height: 200px;
  }

	.fade-in-inline {
		position: absolute;
		text-align: center;
		margin: auto;
		-webkit-mask-image: linear-gradient(to right,
																			transparent 0%,
																			transparent 40%,
																			black 50%,
																			black 90%,
																			transparent 100%);
		mask-image: linear-gradient(to right,
																transparent 0%,
																transparent 40%,
																black 50%,
																black 90%,
																transparent 100%);
		-webkit-mask-size: 8000% 100%;
		mask-size: 8000% 100%;
		animation-name: sweepMask;
		animation-duration: 4s;
		animation-iteration-count: infinite;
		animation-timing-function: linear;
		animation-delay: -1s;
	}

	.fade-in2-inline {
			animation-delay: 1s;
	}

	.inline-div {
			position: relative;
	    display: inline-block; /* Makes both the div and paragraph inline-block elements */
	    vertical-align: top; /* Aligns them at the top, you can adjust this to middle, bottom, etc., based on your needs */
	    width: 50px; /* Optional: Adds space between the div and the paragraph */
	}

	.nav-item {
		display: block;
		margin: 12px 0 4px 15px; /* top right bottom left */
	}

	.listing_items span {
		background-color: #f1f1f6;
		padding: 2px 2px;
		border-radius: 6px;
		font-weight: 400;
		margin-bottom: 0;
	}

	

</style>

	  <title>PointPROP</title>
      <meta property="og:title" content="PointPROP: Parallel Ray Occlusion Prediction for Point Clouds" />
			<meta charset="UTF-8">
  </head>

  <body>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<table class="header" align=center>
								<tr>
									<td align=center colspan=4 style="padding-bottom:1em; padding-top:3em;">
										<span style="font-size: 42px; font-family: 'Open Sans', sans-serif, monospace; font-weight: bold; /* Adds fallbacks */">PointPROP: Parallel Ray Occlusion Prediction for Point Clouds</span>
									</td>
								</tr>
								<tr>
									<td colspan=4 align=center  style="padding-bottom:3em;"><span style="font-size:18px">Final project for 6.7960, MIT</span></td>
								</tr>
								<tr>
										<td align=center>
												<span style="font-size:17px"><a target="_blank" href="https://amberjiayusu.github.io">Amber Jiayu Su</a></span>
										</td>
										<td align=center>
												<span style="font-size:17px"><a target="_blank" href="https://www.linkedin.com/in/erickylecheung/">Eric Kyle Cheung</a></span>
										</td>
								</tr>
								
						</table>
					</div>
		</div>

		<div class="content-margin-container" id="intro">
				<div class="margin-left-block">
          <!-- table of contents here -->
          <div style="position:fixed; max-width:inherit; top:max(20%,120px)">
				<b style="font-size:18px">Outline</b><br><br>
				<a href="#intro">Introduction</a><br>
				<span class="nav-item"><a href="#motivation">Motivation</a></span>
			    <span class="nav-item"><a href="#previous_work">Previous Work</a></span>
				<span class="nav-item"><a href="#task">Task</a></span><br>
				<a href="#experiments">Experiments</a><br>
				<span class="nav-item"><a href="#data">Data</a></span>
			    <span class="nav-item"><a href="#architecture">Overall Architecture</a></span>
				<span class="nav-item"><a href="#experiment">Experiments</a></span>
				<span class="nav-item"><a href="#baseline_comparison">1-Baseline Comparison</a></span>
				<span class="nav-item"><a href="#capacity_analysis">2-Capacity Analysis</a></span>
				<span class="nav-item"><a href="#local_bias">3-Local Bias</a></span><br>
				<a href="#discussion">Discussion</a><br><br>
				<a href="#future_work_and_limitations">Implications and Limitations</a><br><br>
          </div>
			</div>
		    <div class="main-content-block">
            <!--You can embed an image like this:-->
            <img src="./images/your_image_here.png" width=512px/>
		    </div>
		</div>

    <div class="content-margin-container" id="intro">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Introduction</h1>

				<h2 id="motivation" style="margin-top: 20px; margin-bottom: 10px;">Motivation</h2>
				<p>
					The assessment of solar potential and shading is central to climate and energy research, supporting applications such as photovoltaic yield estimation and building energy simulation. While annual metrics are common, they miss critical temporal variation: sky conditions, and solar angles, affecting peak loads and hour-level PV output. Point-in-time radiation is therefore essential for capturing these dynamics.
				<!-- </p>
				<p> -->
					Urban radiation analysis typically relies on 3D models constructed from GIS footprints and airborne LiDAR point clouds. GIS provides 2D building outlines, while LiDAR supplies dense 3D samples of surfaces. Combined, these datasets form the basis for physics-based simulations/analysis that evaluate how solar rays interact with surrounding geometry.
				</p>
				<p>
					However, relying on GIS-based surfaces and reconstructed geometries presents practical limitations: (1) Comprehensive coverage of spatial information cannot depend on GIS alone. (2) Converting raw LiDAR point clouds (rich but non-volumetric samples) into surface or volumetric geometry is computationally expensive and often fails on complex urban forms, sometimes requiring manual cleanup.  (3) Even after preprocessing, point-in-time ray-tracing remains slow and costly.
				</p>
				<p> 
					Therefore, given the increasing availability of large-scale LiDAR data that provides rich 3D context, there is a need for efficiently and directly leveraging raw point clouds to estimate solar radiation at fine temporal scales. This project works towards this by developing a deep-learning-based surrogate model that predicts point-in-time parallel (assuming solar rays are parallel) ray occlusion from unprocessed urban LiDAR point clouds.
				</p>
				<p>
					Given the specificity of this task, we are trying to explore the overarching question:
					<strong>
						"How should we design the inductive biases of a neural surrogate model so that it can accurately approximate
						a physically-defined, geometry-dependent function?"
					</strong>
					Unlike semantic tasks, directional occlusion is a physical mapping governed by local geometry, long-range interactions,
					and directional constraints. The key challenge is determining which architectural inductive biases best capture these 
					properties within a learnable surrogate model.
				</p>
				<p> 
					As deep learning becomes increasingly integrated into physics-based surrogate modeling, our investigation aims to understand how architectural choices influence performance on a highly specific geometric–physical task. We hope that these insights can contribute to a broader discussion of how to design domain-appropriate inductive biases for physics-driven applications.
				</p>

			
				<h2 id="previous_work" style="margin-top: 20px; margin-bottom: 10px;">Previous Work</h2>
				<span style="font-weight: 500; margin-bottom: 0;">Physical-based Simulations / Analytical Methods</span>
				<p>
					Recent research has developed several physical-based simulations or analytical methods to estimate point-in-time parallel ray occlusion using raw point clouds. Bognár et. al. projected the points to a discretized skydome, and trained an SVM classifier to predict occlusion labels for each sky patch based on projected points <a href="#ref_1">[1]</a>. Su and Dogan developed a voxelization-based method that converts point clouds into volumetric grids, enabling efficient ray-voxel intersection tests for occlusion determination <a href="#ref_2">[2]</a>. These methods avoid explicit surface reconstruction but still require computationally intensive preprocessing steps and hand-crafted features, limiting scalability and generalization.
				</p>
				<span style="font-weight: 500; margin-bottom: 0;">ML-based Coarse Resolution Estimation </span>
				<p>
				</p>

				<span style="font-weight: 500; margin-bottom: 0;">Machine Learning for Point Cloud Understanding</span>
				<p>
					Machine Learning for Point Cloud Understanding has been approached through several encoder families, each offering different trade-offs in geometric fidelity, locality modeling, and scalability:<br>
					<div class="listing_items">
						<ul>
							<li>
								<span > MLP-based encoders</span> (PointNet, PointNet++) <br>
								These models operate directly on unordered point sets by applying shared per-point MLPs followed by symmetric pooling operations to ensure permutation invariance. PointNet uses only global pooling to form a scene-level representation, whereas PointNet++ extends this framework with hierarchical neighborhood grouping and local shared-MLP aggregation to capture fine-grained geometric structure.
							</li>

							<li>
								<span > Transformer-based encoders</span> (Point Transformer, Point-BERT).<br>
								Use attention mechanisms to capture long-range interactions and context-aware features. These models achieve strong accuracy but may be computationally expensive for contexts containing large number of points.
							</li>

							<li>
								<span > Lightweight hierarchical encoders</span> (RandLA-Net). <br>
								Use random down-sampling and local feature aggregation to efficiently process large numbers of points while preserving local geometric detail. This class is well-suited for large urban LiDAR datasets.
							</li>

							<li>
								<span > Autoencoder-style encoders</span> (Point-VAE, Point-MAE, Point-BERT pretraining). <br>
								Learn a global latent representation by reconstructing the input point cloud. These models are effective for capturing scene-level structure and provide strong pretraining signals, but reconstruction-focused objectives focus more on semantic understanding than fine-grained geometric details needed for occlusion reasoning.
							</li>
						</ul>
					</div>

				</p>
				<p>
					Visibility prediction / ray-based occlusion learning

					(NeRV, ray classification networks, shadow detection networks) — relevant because solar radiation ≈ “sun visibility probability × intensity.”

					maybe other visibility work?
				</p>

				<h2 id="task" style="margin-top: 20px; margin-bottom: 10px;">Task</h2>
				<p>
					In short, we model this as a binary classification task and develop a surrogate model to approximate the occlusion-testing function, which determines, for each test point in \(\text{pts}_{\text{test}}\), whether a given \(\text{ray}\) is blocked by the surrounding context geometry \(\text{pts}_{\text{context}}\):
				</p>
				<p>
					\[
						f(\text{pts}_{\text{context}},\, \text{pts}_{\text{test}},\, \text{ray}) \; \in \; \{0,1\}^m
					\]
					where
					\(\text{pts}_{\text{context}} \in \mathbb{R}^{n \times 3}\),
					\(\text{pts}_{\text{test}} \in \mathbb{R}^{m \times 3}\), and
					\(\text{ray} \in \mathbb{R}^3\).
				</p>


		    </div>
		</div>

		<div class="content-margin-container" id="experiments">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
				<h1>Experiments</h1>
				<h2 id="data" style="margin-top: 20px; margin-bottom: 10px;">Data</h2>
				<p>
					Urban-scale LiDAR point clouds (often containing billions of points) are partitioned into fixed-size sub-patches under computational constraints. From each sub-patch, we extract (1) context points, (2) test points, (3) a solar ray direction, and (4) binary occlusion labels indicating whether each test point is blocked along that ray. These components form the input–output pairs used to train our model.
				</p>
				<p> WE need some graph here </p>
				<p>
					The ground truth is produced via a previously developed voxelization approach, applied to each point and followed by a subsequent physics based simulation <a href="#ref_1">[1]</a>. Voxelization converts points into a three dimensional grid of cells, with each volumetric cell being solid or empty. Empty cells allow solar rays to pass through. 
				</p>
				<p>
					For each context–test point pair, we run a point-in-time occlusion simulation for 512 rays uniformly sampled over the upper hemisphere, producing binary occlusion labels for every test point under each ray direction (1 for non-occluded, 0 for occluded). To reduce directional and scene-specific overfitting, we randomly sample 16 rays per block during training. The final dataset contains 1,377 training samples, 393 validation samples, and 198 test samples, generated from 123 unique context–test block pairs.
				</p>

				<h2 id="architecture" style="margin-top: 20px; margin-bottom: 10px;">Overall Architecture</h2>
				<p>
					Our model follows a modular, encoder–classifier architecture designed to jointly reason over 3D geometry and solar direction. The point cloud encoder processes both the context cloud and the test-point cloud, producing latent representations that capture local structure, occluders, and spatial relationships relevant to ray–geometry interactions. In parallel, the ray vector is mapped into a learned embedding that serves as a directional conditioning signal. These three latent features (context, test points, and solar token) are then fused and passed to a classifier that predicts per-point occlusion.
				</p>
				<div class ="listing_items"> 
					<ul>
						<li>
							<p>
								<span > Point Cloud Encoder </span> <br>
								Given the scale and density of the urban LiDAR blocks in our dataset, we adopt a RandLA-Net–style encoder, which offers an efficient hierarchical representation that preserves local geometric structure—critical for accurate ray–occlusion prediction. Because our task is not semantic segmentation, we discard RandLA-Net’s decoder and retain only the hierarchical feature extractor. Although transformer-based or generative encoders could also be applied, many are not benchmarked on large-scale urban LiDAR and require more evaluation effort; thus, we focus on the RandLA family for this project. 
							</p>
							<p>
								The RandLA encoder operates through multiple hierarchical stages, each performing random sampling to reduce point count followed by Local Feature Aggregation (LFA), which aggregates geometric context from nearest neighbors using attentive pooling. This produces a multi-scale latent representation consisting of per-point features and their corresponding downsampled coordinates for both context and test point sets.
							</p>
							<p>
								Following the RandLA encoding, we process the downsampled coordinates using a shared positional MLP, optionally preceded by Fourier feature embedding. The resulting positional embeddings are then fused with the RandLA features (via addition), providing each point with both learned geometric features and explicit spatial encoding.
							</p>
							
						</li>

						<li>
							<p>
								<span > Ray Encoder </span> <br>
								A simple MLP maps the 3D ray vector into the hidden-dimension embedding space, producing a solar token that captures directional information relevant to occlusion reasoning. Optional Fourier feature embedding with fewer frequencies than used for point positions can be applied to the ray vector prior to the MLP to enhance high-frequency directional encoding.
							</p>
							
						</li>

						<li>
							<p>
								<span > Classification Head </span> <br>
								After obtaining the context points and test points features (added positional embeddings and encoded features), and solar token, we fuse them to predict per-point occlusion. Specifically, we concatenate the solar token to the context features by broadcasting the solar token across all context points. Then we concatenate the context features with the solar token and pass through a simple fead forward MLP to produce fused context features.
							</p>
							<p>
								We then implement two prediction heads: 
							</p>
							<ol>
								<li>
									The first model is a lightweight MLP that predicts per-point occlusion using globally pooled context features. We follow the PointNet paradigm of using symmetric global pooling to obtain a compact context descriptor. Specifically, mean and max pooling are applied to the context features to form a scene-level embedding, which is then broadcast to all test points. Each test-point feature is concatenated with this global descriptor and passed through a lightweight MLP to produce per-point occlusion predictions.
									<!-- The first is a lightweight MLP classifier that directly predicts per-point occlusion from the fused test-point features. We first extract the context features by applying mean pooling and max pooling over context-point features. The pooled features are concatenated and projected into a compact scene-level embedding, which is then broadcast to every test point. Each test-point feature is concatenated with this global context descriptor and passed through a small MLP to produce the final occlusion prediction. -->
								</li>
								<li>
									The third prediction head incorporates cross-attention between context and test points to enable more explicit geometric reasoning. Here, we treat the test-point features as queries and the fused context features as keys and values in a multi-head attention mechanism. This allows each test point to attend to relevant context points based on learned geometric relationships and the solar direction. The attention output is then concatenated with the original test-point features and passed through an MLP to produce the final occlusion prediction.
								</li>
							</ol>
						</li>

					</ul>
				</div>	
					<p><span style="font-weight: 500;">Loss</span></p>
					<p>We use binary cross-entropy loss to supervise the occlusion predictions against ground-truth labels.</p>
						\[
						\mathcal{L}_{\text{BCEWithLogits}}(x, y)
						=
						-\Big[
						y \,\log \sigma(x)
						+
						(1 - y)\,\log\!\left(1 - \sigma(x)\right)
						\Big]
						\]
				<h2 id="experiment" style="margin-top:20px; margin-bottom:2px;">
					Experiments
				</h2>	
				<p>
					In the following section, we systematically evaluated which inductive biases matter for learning point-in-time ray occlusion from raw urban LiDAR. Stage 1 compared architectural families (MLP vs. transformer) with and without Fourier features. Stage 2 varied geometric resolution (8× → 4× downsampling) and model capacity. Stage 3 introduced a PointNet++-inspired local pooling module on top of the global pooling based MLP.
				</p>
				
				<h2 id="baseline_comparison" style="margin-top:20px; margin-bottom:2px;">
					1. Baseline Comparison
				</h2>
				<p style="font-size:16px; color:#666; margin-top:0; margin-bottom:2px;">
					Baseline Evaluation of Architectural Inductive Biases (Transformer vs. MLP, ± Fourier)
				</p>
				<p><span style="font-weight: 500;">Set Up</span></p>
				<p>
				First, we establish a baseline comparison to examine the inductive biases of different architectural families. We evaluate transformer-based and MLP-based encoders, each tested with and without Fourier positional embeddings. This setup allows us to assess how these modeling choices affect the model’s ability to learn a geometry-dependent occlusion function. Transformers are expected to capture longer-range interactions and contextual relationships, whereas MLPs with global pooling primarily learn coarse global structure and are less effective at modeling fine geometric detail. Adding Fourier features introduces high-frequency positional encoding that can improve sensitivity to subtle geometric variations.
				</p>
				<p>
				For the transformer-based encoder, we implement a 4-head attention module with a single cross-attention block. For the MLP-based encoder, we use a 5-layer MLP with hidden dimension 128. For comparison with spatially aware point-based architectures, we also include a RandLA-Net encoder with three layers and a downsampling ratio of 2 at each layer (8× total reduction), using a final feature dimension of 256.
				</p>
				<p>
				All models are trained for 20 epochs with batch size 8 using the Adam optimizer (learning rate \(1\times10^{-4}\), weight decay \(1\times10^{-5}\)). A <code>ReduceLROnPlateau</code> scheduler (mode = "min") halves the learning rate when the validation loss does not improve for 5 epochs. We evaluate performance using accuracy, precision, recall, and F1-score on the validation and test sets.
				</p>
				<p><span style="font-weight: 500;">Results</span></p>
				<p>
					Figure X shows the training and validation loss trajectories for the four model architectures. Across all models, training loss decreases smoothly and validation loss remains generally aligned, indicating no severe overfitting within 20 epochs. Fourier features consistently improve convergence, particularly for the MLP, yielding lower loss compared to their non-Fourier counterparts. However, the Fourier embeddings also introduce noticeably higher variance in the validation loss, suggesting that the added high-frequency capacity makes the models more sensitive to noise and local fluctuations during training. Overall, MLP models converge marginally more smoothly and reliably than transformer-based models, and the MLP with Fourier features attains the lowest final loss.
				</p>
				<figure style="text-align: center;">
				<img src="./images/training_validation_loss_comparison.png" width="80%" />
				<figcaption><span style="font-weight: 500;">Figure X.</span> Training and validation loss curves for the four model architectures.</figcaption>
				</figure>
				<p>
					Table X. summarizes the test set performance across the four model configurations. Across all four configurations, precision, recall, and F1 are nearly identical, indicating that neither the architectural family nor positional encodings meaningfully alter the classification trade-offs. The models systematically yields high recall and moderate precision, suggesting a bias towards predicting non-occluded points. 
				</p>
				<div style="text-align: center; margin-top: 20px;">
				<table 
					style="
						border-collapse: collapse; 
						margin-left: auto; 
						margin-right: auto; 
						text-align: center; 
						font-size: 15px; 
						font-weight: 300;
						border: 1.5px solid #000;
					"
				>
					
					<caption style="caption-side: bottom; ; margin-top: 8px;">
						<span style="font-weight: 500;">Table X.</span>  Model Performance Comparison
					</caption>

					<thead>
						<tr style="border-bottom: 1.5px solid #000;">
							<th style="padding: 8px 20px;">Model</th>
							<th style="padding: 8px 20px; border-right: 1px solid #000;"">Fourier</th>
							<th style="padding: 8px 20px;">Accuracy</th>
							<th style="padding: 8px 20px;">Precision</th>
							<th style="padding: 8px 20px;">Recall</th>
							<th style="padding: 8px 20px;">F1 Score</th>
							<th style="padding: 8px 20px;"># Params</th>
						</tr>
					</thead>

					<tbody>
						<!-- MLP -->
						<tr>
							<td rowspan="2" style="font-weight: 500; padding: 8px 20px;">MLP</td>
							<td style="padding: 8px 20px; border-right: 1px solid #000;">With</td>
							<td style="padding: 8px 20px;">0.7462</td>
							<td style="padding: 8px 20px;">0.7737</td>
							<td style="padding: 8px 20px;">0.8672</td>
							<td style="padding: 8px 20px;">0.8178</td>
							<td style="padding: 8px 20px;">761,761</td>
						</tr>
						<tr>
							<td style="padding: 8px 20px; border-right: 1px solid #000;">Without</td>
							<td style="padding: 8px 20px;">0.7357</td>
							<td style="padding: 8px 20px;">0.7523</td>
							<td style="padding: 8px 20px;">0.8907</td>
							<td style="padding: 8px 20px;">0.8157</td>
							<td style="padding: 8px 20px;">758,305</td>
						</tr>

						<!-- Transformer -->
						<tr>
							<td rowspan="2" style="font-weight: 500; padding: 8px 20px;">Transformer</td>
							<td style="padding: 8px 20px; border-right: 1px solid #000;">With</td>
							<td style="padding: 8px 20px;">0.7360</td>
							<td style="padding: 8px 20px;">0.7630</td>
							<td style="padding: 8px 20px;">0.8675</td>
							<td style="padding: 8px 20px;">0.8119</td>
							<td style="padding: 8px 20px;">942,625</td>
						</tr>
						<tr>
							<td style="padding: 8px 20px; border-right: 1px solid #000;">Without</td>
							<td style="padding: 8px 20px;">0.7414</td>
							<td style="padding: 8px 20px;">0.7657</td>
							<td style="padding: 8px 20px;">0.8734</td>
							<td style="padding: 8px 20px;">0.8160</td>
							<td style="padding: 8px 20px;">939,169</td>
						</tr>
					</tbody>

				</table>				
			</div>
			<p>
				Figure X shows one example from the test set, comparing ground-truth occlusion labels with the MLP model predictions (with Fourier features). Visually, the model captures the global occlusion patterns well, correctly identifying regional patterns. However, finer details are missed, such as the it predicts some small occluded clusters as a whole band of occuluded region. This suggests that while the model learns the overall geometry-dependent occlusion function, it struggles with fine-grained geometric reasoning, likely due to the aggressive downsampling and limited local context modeling inherent in the MLP architecture.
			</p>
			<figure style="text-align: center;">
			<img src="./images/samples_grid_basic_blog.png" width="80%" />
			<figcaption><span style="font-weight: 500;">Figure X.</span> Testing Example with MLP + Fourier</figcaption>
			</figure>
			


				



				<h2 id="capacity_analysis" style="margin-top: 20px; margin-bottom: 2px;">2. Capacity Analysis</h2>
				<p style="font-size:16px; color:#666; margin-top:0; margin-bottom:10px;">
					Capacity and Resolution Analysis via Downsampling and Feature Reduction
				</p>
				<p><span style="font-weight: 500;">Set Up</span></p>
				<p>
					Since the initial experiments did not reveal meaningful differences between architectural families or the use of positional encodings, we hypothesize that the performance bottleneck lies not in the encoder choice itself but in the information loss introduced by the aggressive downsampling of the context point cloud. An overall reduction factor of 8× may remove fine-grained geometric structure that is critical for distinguishing visible from occluded points. At the same time, because this task does not require high-level semantic abstraction, the full representational capacity of the original RandLA-Net may be unnecessary. Reducing the feature dimensionality therefore serves two purposes: it probes our hypothesis that additional capacity does not materially improve performance, and it ensures the encoder remains space-efficient, which is critical given the high memory cost of point-cloud representations.
				</p>
				<p>
					To evaluate whether model performance is constrained by insufficient geometric resolution or by excess representational capacity, we construct two controlled variants of the RandLA-Net encoder that differ systematically in depth, downsampling schedule, and feature dimensionality. These configurations allow us to disentangle the influence of spatial resolution from that of model capacity while respecting the computational and memory constraints inherent to point cloud models. Since the previous experiments showed Fourier features provided some benefits, we use MLP and transformer models with Fourier features as base configurations.
				</p>
				<p style="font-style: italic;"> 
					Note: RandLA-Net expands the feature width within each layer, producing embeddings larger than the nominal dimension; in practice, the effective feature dimensions double relative to the configured outputs.
				</p>
				<div style="text-align: center; margin-top: 20px;">
				<table 
					style="
					border-collapse: collapse;
					margin-left: auto; 
					margin-right: auto;
					font-size: 15px;
					font-weight: 300;
					border: 1.5px solid #000;
					text-align: center;
					"
				>
					<caption style="caption-side: bottom; margin-top: 8px; ">
						<span style="font-weight: 500;">
						Table X. </span>Comparison of RandLA-Net Encoder Configurations
					</caption>

					<thead>
					<tr style="border-bottom: 1.5px solid #000;">
						<th style="padding: 8px 18px; border-right: 1px solid #000;">Configuration</th>
						<th style="padding: 8px 18px;"># Layers</th>
						<th style="padding: 8px 18px;">Subsampling Ratios</th>
						<th style="padding: 8px 18px;">Overall Reduction</th>
						<th style="padding: 8px 18px;">Feature Dimensions</th>
						<th style="padding: 8px 18px;">Final Embedding</th>
					</tr>
					</thead>

					<tbody>
					<tr>
						<td style="padding: 8px 18px; font-weight: 500; border-right: 1px solid #000;">Original Model</td>
						<td style="padding: 8px 18px; ">3</td>
						<td style="padding: 8px 18px;">[2, 2, 2]</td>
						<td style="padding: 8px 18px;">8×</td>
						<td style="padding: 8px 18px;">[16, 64, 128]</td>
						<td style="padding: 8px 18px;">256</td>
					</tr>

					<tr>
						<td style="padding: 8px 18px; font-weight: 500; border-right: 1px solid #000;">Shallow Model</td>
						<td style="padding: 8px 18px;">2</td>
						<td style="padding: 8px 18px;">[2, 2]</td>
						<td style="padding: 8px 18px;">4×</td>
						<td style="padding: 8px 18px;">[32, 64]</td>
						<td style="padding: 8px 18px;">128</td>
					</tr>
					</tbody>
				</table>
				</div>
				<p><span style="font-weight: 500;">Results</span></p>
				<figure style="text-align: center;">
				<img src="./images/training_validation_loss_comparison_2.png" width="80%" />
				<figcaption><span style="font-weight: 500;">Figure X.</span> Training and validation loss curves for the different encoder configurations.</figcaption>
				</figure>
				<p>
					Reducing the downsampling ratio has very different effects on the two encoder families. For the MLP encoder, providing more context points (4× vs. 8× downsampling) leads to smoother and more stable validation curves. Because the we explicitly used global pooling, its effective capacity remains limited regardless of input resolution, i.e. more points simply yield a more reliable global descriptor, making overfitting unlikely. In contrast, the transformer encoder becomes noticeably less stable at higher resolution: training loss continues to decrease while validation loss diverges. This reflects the fact that transformer cross-attention operates on local neighborhoods; increasing point density exposes the model to richer fine-grained geometric patterns and substantially increases its effective capacity, making it prone to memorizing local configurations rather than learning generalizable occlusion cues. Notably, reducing the feature dimensionality does not appear to harm performance for either architecture, further suggesting that the primary bottleneck lies in geometric information rather than representational capacity.
				</p>
				<div style="text-align: center; margin-top: 20px;">
				<table 
					style="
						border-collapse: collapse; 
						margin-left: auto; 
						margin-right: auto; 
						text-align: center; 
						font-size: 15px; 
						font-weight: 300;
						border: 1.5px solid #000;
					"
				>
					
					<caption style="caption-side: bottom; ; margin-top: 8px;">
						<span style="font-weight: 500;">Table X.</span>  Encoder Configuration Performance Comparison
					</caption>

					<thead>
						<tr style="border-bottom: 1.5px solid #000;">
							<th style="padding: 8px 20px;">Model</th>
							<th style="padding: 8px 20px; border-right: 1px solid #000;"">Downsampling</th>
							<th style="padding: 8px 20px;">Accuracy</th>
							<th style="padding: 8px 20px;">Precision</th>
							<th style="padding: 8px 20px;">Recall</th>
							<th style="padding: 8px 20px;">F1 Score</th>
							<th style="padding: 8px 20px;"># Params</th>
							<th style="padding: 8px 20px;">Inference Latency (ms per sample)</th>
						</tr>
					</thead>

					<tbody>
						<!-- MLP -->
						<tr>
							<td rowspan="2" style="font-weight: 500; padding: 8px 20px;">MLP</td>
							<td style="padding: 8px 20px; border-right: 1px solid #000;">8x</td>
							<td style="padding: 8px 20px;">0.7462</td>
							<td style="padding: 8px 20px;">0.7737</td>
							<td style="padding: 8px 20px;">0.8672</td>
							<td style="padding: 8px 20px;">0.8178</td>
							<td style="padding: 8px 20px;">761,761</td>
							<td style="padding: 8px 20px;">231.64</td>
						</tr>
						<tr>
							<td style="padding: 8px 20px; border-right: 1px solid #000;">4x</td>
							<td style="padding: 8px 20px;">0.7458</td>
							<td style="padding: 8px 20px;">0.7743</td>
							<td style="padding: 8px 20px;">0.8649</td>
							<td style="padding: 8px 20px;">0.8171</td>
							<td style="padding: 8px 20px;">204,257</td>
							<td style="padding: 8px 20px;">184.59</td>
						</tr>

						<!-- Transformer -->
						<tr>
							<td rowspan="2" style="font-weight: 500; padding: 8px 20px;">Transformer</td>
							<td style="padding: 8px 20px; border-right: 1px solid #000;">8x</td>
							<td style="padding: 8px 20px;">0.7360</td>
							<td style="padding: 8px 20px;">0.7630</td>
							<td style="padding: 8px 20px;">0.8675</td>
							<td style="padding: 8px 20px;">0.8119</td>
							<td style="padding: 8px 20px;">942,625</td>
							<td style="padding: 8px 20px;">231.32</td>
						</tr>
						<tr>
							<td style="padding: 8px 20px; border-right: 1px solid #000;">4x</td>
							<td style="padding: 8px 20px;">0.7277</td>
							<td style="padding: 8px 20px;">0.7357</td>
							<td style="padding: 8px 20px;">0.9133</td>
							<td style="padding: 8px 20px;">0.8150</td>
							<td style="padding: 8px 20px;">249,633</td>
							<td style="padding: 8px 20px;">186.71</td>
						</tr>
					</tbody>

				</table>				
			</div>
			<p>
				Beyond accuracy, reducing the downsampling ratio and feature dimensionality yields meaningful efficiency gains. As shown in Table X, moving from 8× to 4× downsampling reduces parameter counts by around 40% for both architectures, with a corresponding decrease in inference latency from ~231 ms to ~185 ms per sample (measured on a Google Colab L4 GPU). While reducing downsampling increases the number of input points, the shallower architecture and lower feature dimensions more than compensate, resulting in a net efficiency improvement. As a result, the overall model becomes smaller and faster despite operating on a denser point cloud. While the results with 4x downsampling of MLP encoders are comparable to those with 8x downsampling, the efficiency gains and stability make this configuration more favorable in practice.
			</p>
				<p>
					Additional experiments with more aggressive reductions in both downsampling ratio and feature dimensionality (not shown in Table X) were conducted to assess whether the MLP encoder could be pushed to an even more compact configuration. While the losses remain stable under these more extreme settings, its performance drops noticeably and does not reach the accuracy levels achievable at 4× downsampling. Therefore, we find this configuration to be a reasonable one that yields stable performance. 
				</p>
				<figure style="text-align: center;">
				<img src="./images/samples_grid_4downsample_blog.png" width="80%" />
				<figcaption><span style="font-weight: 500;">Figure X.</span> Testing Example with MLP 4x Downsampling</figcaption>
				</figure>
				<p>
					We visualize the same test example from earlier in Figure X now with MLP with 4x downsampling ratio. Though it still struggles with local patterns, the confidence of the false negative is noticeably lower as indicated in the probability map. This suggests that the model is better able to capture local geometric cues that influence occlusion, even if it does not fully resolve all fine-grained details. 
				</p>


				<h2 id="local_bias" style="margin-top: 20px; margin-bottom: 2px;">3. Local Bias</h2>
				<p style="font-size:16px; color:#666; margin-top:0; margin-bottom:10px;">
					Incorporating Local Geometric Bias for Fine-grained Accuracy
				</p>
				<p>
					Overall, these findings indicate that the 4×-downsampling configuration offers a favorable balance between accuracy, stability, and inference efficiency. At the same time, the experiments suggest the model still struggles to fully capture the local geometric patterns. While the global pooling strategy provides a robust scene-level descriptor and serves as a relibale regularizer to prevent overfitting, it is inherently the bottleneck of the model’s capacity to reason about fine-grained geometric structure. This is particularly important for occlusion reasoning, where local surface orientations, point distributions, and small-scale features can significantly influence visibility along a ray. Therefore, in this section, we introduce and explore a local pooling mechanism adapted from PointNet++ to enhance the model’s capacity to deal with local geometric variations.
				</p>
				<span style="font-weight: 500;">Set Up</span>
				<p>
					To overcome the limitations of purely global pooling, we augment the MLP encoder with a lightweight PointNet++ inspired local aggregation module. The key idea is to introduce a local point level net, while retaining the global context pooling.  
				</p>
				<p>
					Specifically, after the context and test encoders produce feature–coordinate pairs <span style="white-space: nowrap;">\((\mathbf{f}_i^{\text{ctx}}, \mathbf{x}_i^{\text{ctx}})\)</span> and <span style="white-space: nowrap;">\((\mathbf{f}_j^{\text{test}}, \mathbf{x}_j^{\text{test}})\)</span>, (for test points, the final feature is the sum of the feature embedding and the positional encoding; for context points, we additionally fuse this representation with the ray embedding). We then construct a global branch by applying mean and max pooling over all solar-conditioned context features, followed by a linear projection, yielding a scene-level descriptor that is broadcast to all test points. 
				</p>
				<p>
					In parallel, we introduce a local branch that follows the PointNet++ pattern of k-NN grouping, shared MLP, and symmetric pooling. For each test point <span style="white-space: nowrap;">\(\mathbf{x}_j^{\text{test}}\)</span>, we find its K nearest context points in Euclidean space, gather their features and coordinates, and compute relative positions <span style="white-space: nowrap;">\(\Delta \mathbf{x}_{jk} = \mathbf{x}_{k}^{\text{ctx}} - \mathbf{x}_{j}^{\text{test}}\)</span>. 
					These offsets are linearly embedded into a hidden relative-position space and concatenated with the corresponding neighbor features. A shared MLP is then applied to these concatenated vectors to jointly encode the local information. Mean and max pooling over the K neighbors are subsequently applied, then following a linear projection, yielding a local descriptor for each test point.
				</p>
				<p>
					The final representation for each test point is the concatenation of (1) its own test-point feature, (2) the local K-NN pooled feature, and (3) the broadcast global context feature. This combined descriptor is passed through a shallow MLP head to predict per-point occlusion logits.
				</p>
				<span style="font-weight: 500;">Results</span>
				<figure style="text-align: center;">
				<img src="./images/training_validation_loss_comparison_final.png" width="80%" />
				<figcaption><span style="font-weight: 500;">Figure X.</span> Training and validation loss curves with vs without local pooling.</figcaption>
				</figure>
				<p>
					Noticeably, by introducing local pooling, the model achieves smoother and much lower training and validation loss curves, as shown in Figure X. The local aggregation allows the model to better capture fine-grained geometric patterns that influence occlusion than relying on global features along. This leads to more stable learning dynamics and improved generalization, as evidenced by the lower validation loss throughout training.
				</p>
				
				<div style="text-align: center; margin-top: 20px;">
				<table 
					style="
						border-collapse: collapse; 
						margin-left: auto; 
						margin-right: auto; 
						text-align: center; 
						font-size: 15px; 
						font-weight: 300;
						border: 1.5px solid #000;
					"
				>
					
					<caption style="caption-side: bottom; ; margin-top: 8px;">
						<span style="font-weight: 500;">Table X.</span>  Local Pooling Performance Comparison
					</caption>

					<thead>
						<tr style="border-bottom: 1.5px solid #000;">
							<th style="padding: 8px 20px;  border-right: 1px solid #000;">Model</th>
							<th style="padding: 8px 20px;">Accuracy</th>
							<th style="padding: 8px 20px;">Precision</th>
							<th style="padding: 8px 20px;">Recall</th>
							<th style="padding: 8px 20px;">F1 Score</th>
							<th style="padding: 8px 20px;"># Params</th>
							<th style="padding: 8px 20px;">Inference Latency (ms per sample)</th>
						</tr>
					</thead>

					<tbody>
						<!-- MLP -->
						<tr>
							<td  style="padding: 8px 20px; border-right: 1px solid #000;">MLP with Global Pooling</td>
							<td style="padding: 8px 20px;">0.7458</td>
							<td style="padding: 8px 20px;">0.7743</td>
							<td style="padding: 8px 20px;">0.8649</td>
							<td style="padding: 8px 20px;">0.8171</td>
							<td style="padding: 8px 20px;">204,257</td>
							<td style="padding: 8px 20px;">184.59</td>
						</tr>
						<tr>
							<td style="padding: 8px 20px; border-right: 1px solid #000;">MLP with Global and Local Pooling</td>
							<td style="padding: 8px 20px;">0.8320</td>
							<td style="padding: 8px 20px;">0.8516</td>
							<td style="padding: 8px 20px;">0.9011</td>
							<td style="padding: 8px 20px;">0.8757</td>
							<td style="padding: 8px 20px;">266,337</td>
							<td style="padding: 8px 20px;">184.07</td>
						</tr>

					</tbody>

				</table>				
			</div>
			<p>
				We also report the testing performance in Table X. Since architecturally-wise, we are only introducing one shared relative position projection and local point MLP, the parameter count remains nearly identical to the baseline MLP model. However, the addition of local pooling yields consistent improvements across all metrics, including accuracy, precision, recall, and F1-score. This confirms that incorporating local geometric context enhances the model’s ability to reason about occlusion, leading to more accurate predictions without significantly increasing model complexity.
			</p>
			<figure style="text-align: center;">
				<img src="./images/samples_grid_localPooling_blog.png" width="80%" />
				<figcaption><span style="font-weight: 500;">Figure X.</span> Training and validation loss curves with vs without local pooling.</figcaption>
				</figure>
			<p>
				Looking at the same test example in Figure X, we see that the model with local pooling recovers much finer details in the occlusion patterns. Many of the small occluded clusters that were previously misclassified are now correctly identified, and the overall confidence map appears much sharper and more precise. The probability map shows higher confidence in both occluded and non-occluded regions, indicating that the model is more certain about its predictions. This demonstrates that the local pooling mechanism effectively enhances the model’s sensitivity to fine-grained geometric cues that are critical for accurate occlusion reasoning.
			</p>
				



				  	
		    </div>
		</div>

		<div class="content-margin-container">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
            <h1>Discussion</h1>
				<p>
					A key insight from our experiments is that the strongest performance does not come from using the most expressive or capacity-heavy backbone, but from structuring the model so that its inductive biases reflect the physics of the task. The global-only model performs surprisingly well because it offers a stable, coarse summary of the entire scene while remaining simple enough to act as a regularizer, preventing the model from overfitting the highly irregular fine-scale patterns in point clouds. 
				</p>
				<p>
					However, global pooling alone cannot capture the local geometric cues that fundamentally determine whether a ray intersects nearby surfaces. The local pooling module succeeds because it adds precisely the missing part: a lightweight, shared, spatially grounded representation of neighborhood-scale geometry, while the global branch continues to anchor the model with a broad contextual overview. This balance—global regularization plus local geometric sensitivity which appears far more important than maximizing parameter count or adopting a more complex architecture like a transformer. 
				</p>
				<p>
					More broadly, in point-cloud tasks where both data density and memory footprint impose hard constraints, performance hinges on what geometric structure the model is biased to extract, not on raw representational power. Designing architectures that elevate the physically relevant aspects of the task, such as local occluders, directional relationships, multi-scale structure, should be higher priorities than simply scaling up model size or complexity.
				</p>
		    </div>
		</div>

		<div class="content-margin-container" id="implications_and_limitations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<h1>Implications and Limitations</h1>
						<h2>
							Implications
						</h2>
						<p>
							From a deep learning perspective, these experiments demonstrate that carefully designed inductive biases—not larger or more complex backbones—are the key to learning fine-grained, physics-driven functions directly from large, noisy point clouds. By combining a lightweight global pooling branch that stabilizes learning with a targeted local aggregation module that captures neighborhood-scale geometry, we show how to construct an efficient surrogate model that aligns with the underlying ray-occlusion physics while remaining memory-feasible for urban-scale data. 
						</p>
						<p>
							From an application standpoint, this work provides a practical pathway toward fast, point-in-time solar occlusion estimation without requiring expensive geometric reconstruction or heavy ray-tracing. This enables more scalable radiation analysis across cities and supports downstream workflows in PV potential mapping, shading assessment, and urban energy modeling—tasks that increasingly demand fine temporal resolution but are constrained by the computational cost of traditional simulation pipelines.
						</p>
						<h2>
							Limitations
						</h2>
						<p>
							Several limitations of the current study point toward important directions for future work. 
						</p>
						<p>
							First, although we systematically explored architectural families, downsampling ratios, and local–global fusion, many other design parameters remain untested, such as alternative neighborhood definitions, different positional encodings, or more advanced context–test interaction mechanisms. These choices likely matter, but the search space is too large to fully explore within the scope of this project. 
						</p>
						<p>
							Second, because point-cloud models are extremely memory-intensive, an essential challenge is developing architectures that remain lightweight without sacrificing task-specific capacity. Our current design borrows standard point-cloud operations (RandLA-style sampling, PointNet++ grouping), but these may not be optimal for large urban scenes; future models need more deliberate architectural efficiency rather than simply adapting existing modules. 
						</p>
						<p>
							Third, the training data, while large, may not be diverse enough to cover the real-world simulation, as they come from a narrow set of urban contexts with similar point densities and acquisition characteristics. This limits the model’s ability to generalize to different cities, scanning conditions, or more varied geometric environments. Finally, the ground-truth occlusion labels themselves arise from a voxelization-based simulation that introduces noise and discretization errors. Understanding how to make the surrogate robust to noisy labels, or how to design a data generation pipeline that provides more reliable fine-scale supervision, remains an open problem.
						</p>
		    </div>
		</div>

		<div class="content-margin-container" id="citations">
				<div class="margin-left-block">
				</div>
		    <div class="main-content-block">
						<div class='citation' id="references" style="height:auto"><br>
							<span style="font-size:16px">References:</span><br><br>
							<a id="ref_1"></a>[1] <a target="_blank" href="https://doi.org/10.1080/19401493.2021.1971765">Calculating Solar Irradiance without Shading Geometry: a Point Cloud-based Method</a>, Bognár, Loonen, and Hensen, 2021<br><br>
							<a id="ref_2"></a>[2] <a target="_blank" href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5611022">Voxel Shader – An Efficient Shadow Calculation Approach For Urban Building Energy Modeling</a>, Su, and Dogan, 2025<br><br>
							<a id="ref_2"></a>[2] <a href="">A Human-Level AGI</a>, OpenAI, 2025<br><br>
						</div>
		    </div>
		</div>

	</body>

</html>



